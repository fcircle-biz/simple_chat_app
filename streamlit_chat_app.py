import streamlit as st
from langchain_ollama import ChatOllama
from langchain.schema import HumanMessage, AIMessage

from langchain_community.tools.tavily_search import TavilySearchResults
import os

st.title("Qwen3:1.7b Chat App ï¼‹ Webæ¤œç´¢")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ Tavily APIã‚­ãƒ¼å–å¾—
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
if not TAVILY_API_KEY:
    st.error("âŒ ç’°å¢ƒå¤‰æ•° TAVILY_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

search = TavilySearchResults(api_key=TAVILY_API_KEY)

# ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
if "chat" not in st.session_state:
    st.session_state.chat = ChatOllama(model="qwen3:1.7b")

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# ğŸ”˜ Webæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
web_mode = st.checkbox("ğŸŒ Webæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆAIãŒè¦ç´„ï¼‰", value=False)

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã®è¡¨ç¤º
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg["role"] == "assistant":
            with st.expander("Response Details"):
                st.markdown(msg["content"])
        else:
            st.markdown(msg["content"])

# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- Webæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ ---
    if web_mode:
        # â‘  Tavilyã§æ¤œç´¢
        search_result = search.run(prompt)

        # â‘¡ æ¤œç´¢çµæœã‚’Qwen3ã«è¦ç´„ä¾é ¼
        summarization_prompt = f"æ¬¡ã®Webæ¤œç´¢çµæœã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã«ã‚ã‹ã‚Šã‚„ã™ãç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{search_result}"
        message_placeholder = st.empty()
        full_summary = ""

        for chunk in st.session_state.chat.stream([HumanMessage(content=summarization_prompt)]):
            full_summary += chunk.content
            message_placeholder.markdown(full_summary + "â–Œ")

        import re
        cleaned_summary = re.sub(r'<think>.*?</think>', '', full_summary, flags=re.DOTALL)
        message_placeholder.markdown(cleaned_summary)

        st.session_state.messages.append({"role": "assistant", "content": cleaned_summary})

    else:
        # --- é€šå¸¸ã®AIãƒãƒ£ãƒƒãƒˆå‡¦ç† ---
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            for chunk in st.session_state.chat.stream([HumanMessage(content=prompt)]):
                full_response += chunk.content
                message_placeholder.markdown(full_response + "â–Œ")

            import re
            cleaned_response = re.sub(r'<think>.*?</think>', '', full_response, flags=re.DOTALL)
            message_placeholder.markdown(cleaned_response)

            st.session_state.messages.append({"role": "assistant", "content": cleaned_response})
